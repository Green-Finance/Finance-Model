import time
import pandas as pd

# Modules 
from src.parser import url_parser
from src.extractor import Extractor
from src.data_insert import PGVecInsert
from src.document_chunking import chunking_documents
from src.load_embedding_model import load_embedding_model

# PG Store 
from langchain_postgres import PGVector

# ìˆ˜ì§‘ ì¹´í…Œê³ ë¦¬ ì„¤ì •
category_lst_main = [
    "https://finance.naver.com/research/market_info_list.naver",
    "https://finance.naver.com/research/market_info_list.naver",
    "https://finance.naver.com/research/economy_list.naver",
    "https://finance.naver.com/research/economy_list.naver",
    "https://finance.naver.com/research/debenture_list.naver"
]

category_lst_industry = [
    "https://finance.naver.com/research/company_list.naver",
    "https://finance.naver.com/research/industry_list.naver"
]

# ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ìˆ˜ì§‘
def fetch_list_data(url, page):
    @url_parser(url=url, pages=page)
    def wrapped(response):
        extract = Extractor(response)
        return extract.extract_element()
    return wrapped()

def fetch_plus_list_data(url, page):
    @url_parser(url=url, pages=page)
    def wrapped(response):
        extract = Extractor(response)
        return extract.extract_industry_stockitems()
    return wrapped()

# ìƒì„¸ ë³¸ë¬¸ ìˆ˜ì§‘
def fetch_detail_content(detail_url):
    @url_parser(url=detail_url, pages=1)
    def wrapped(response):
        extract = Extractor(response)
        return extract.detail_page_crawler()
    return wrapped()

# ğŸš€ ë©”ì¸ ì‹¤í–‰
def main():
    final_data = []

    print("\n [1] ë„¤ì´ë²„ ë¦¬ì„œì¹˜ ë¬¸ì„œ ìˆ˜ì§‘ ì‹œì‘")
    for url in category_lst_main:
        list_data = fetch_list_data(url, page=1)
        for item in list_data:
            content = fetch_detail_content(item["link"])
            item["content"] = content
            item["items"] = None  # ì¢…ëª© ì—†ìŒ
            final_data.append(item)
            time.sleep(3)

    for url in category_lst_industry:
        list_data = fetch_plus_list_data(url, page=1)
        for item in list_data:
            content = fetch_detail_content(item["link"])
            item["content"] = content
            final_data.append(item)
            time.sleep(3)

    df = pd.DataFrame(final_data)
    print(f"ì´ ìˆ˜ì§‘ ë¬¸ì„œ ìˆ˜: {len(df)}ê±´")

    # DB ì„¤ì •
    db_config = {
        "dbname": "mydatabase",
        "user": "myuser",
        "password": "mypassword",
        "host": "localhost",
        "port": 5432
    }

    # ì„ë² ë”© ëª¨ë¸ ë¡œë”©
    embedding_model = load_embedding_model()

    # ë²¡í„°ìŠ¤í† ì–´ ì—°ê²°
    vectorstore = PGVector(
        connection="postgresql+psycopg2://myuser:mypassword@localhost:5432/mydatabase",
        collection_name="industry_reports",
        embeddings=embedding_model,
        use_jsonb=True
    ) # ìš” ë¶€ë¶„ì´ ë‚´ ìƒê°ì—” í•˜ë“œì½”ë”© ê°™ê¸°ë„ í•˜ê³  ... ì™œëƒë©´ db_configë‘ ê°™ì´ ë˜ connection ëì–ì•„

    # ì¸ì„œíŠ¸ í´ë˜ìŠ¤ ìƒì„± ë° ì‚½ì…
    inserter = PGVecInsert(
        db_config=db_config,
        embedding_model=embedding_model,
        vectorstore=vectorstore
    )

    inserter.insert_dataframe(df, chunking_fn=chunking_documents)

    print(f"\nâœ… ì´ {len(df)}ê±´ ì²˜ë¦¬ ë° ì €ì¥ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
