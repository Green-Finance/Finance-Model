import pandas as pd
import psycopg2
from psycopg2.extras import execute_values
from .document_chunking import chunking_documents
import json 


class PGVecInsert:
    def __init__(self, db_config: dict):
        self.db_config = db_config

    def _get_connection(self):
        return psycopg2.connect(**self.db_config)

    def insert_dataframe(self, df: pd.DataFrame, table_name: str = "industry_reports"):
        insert_query = f"""
        INSERT INTO {table_name} (
            collection_name, title, link, pdf, stock, date, item,
            content, document, embedding, cmetadata
        ) VALUES %s
        """

        records = []
        print(f"ğŸ”„ ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(df)}")

        for idx, row in df.iterrows():
            print(f"\nğŸ“„ [{idx+1}] ì²˜ë¦¬ ì¤‘: {row['title'][:30]}...")

            content_str = "\n".join(row["content"]) if isinstance(row["content"], list) else row["content"]

            try:
                print(f"ğŸ“¥ PDF ì„ë² ë”© ì¤‘... ({row['pdf']})")
                chunks = chunking_documents(row["pdf"])  # List of (chunk_text, vector)
                print(f"âœ… ì´ ì²­í¬ ìˆ˜: {len(chunks)}")
            except Exception as e:
                print(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨: {row['pdf']} / ì—ëŸ¬: {e}")
                continue
            
            # ì²­ê¹…ëœ ë°ì´í„°, ë²¡í„° ë°ì´í„° - > ë©”íƒ€ë°ì´í„°ë¡œ ë§¤í•‘ langchain Vector searchë¡œ ë³´ê¸° ìœ„í•´ 
            for i, (chunk_text, vector) in enumerate(chunks):
                metadata = {
                    "title": row["title"],
                    "link": row["link"],
                    "pdf": row["pdf"],
                    "stock": row["stock"],
                    "date": row["date"],
                    "item": row["items"]
                }

                records.append((
                    "industry_reports",          # collection_name
                    row["title"],
                    row["link"],
                    row["pdf"],
                    row["stock"],
                    row["date"],
                    row["items"],
                    content_str,
                    chunk_text,                 # document
                    vector,                     # embedding (VECTOR(768))
                    json.dumps(metadata)        # cmetadata (JSONB)
                ))

                if i == 0:
                    print(f"ğŸ“ ì²« ë²ˆì§¸ ì²­í¬: {chunk_text[:40]}...")

        print(f"\nğŸ“¦ ìµœì¢… ì‚½ì… ë ˆì½”ë“œ ìˆ˜: {len(records)}")